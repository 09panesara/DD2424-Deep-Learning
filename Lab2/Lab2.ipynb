{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CIFAR10Data, montage\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Layer Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneLayerClassifer:\n",
    "    def __init__(self, n_classes, input_dim, n_hidden, batch_size=100, eta=0.001, n_epochs=20, lamda=0):\n",
    "        self.batch_size = batch_size # number of images in a batch\n",
    "        self.eta = eta\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lamda = lamda\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # initialise weight matrix and bias\n",
    "        self.W1 = np.random.normal(loc=0, scale=1/np.sqrt(input_dim), size=(n_hidden, input_dim))  # m x d\n",
    "        self.W2 = np.random.normal(loc=0, scale=1/np.sqrt(n_hidden), size=(n_classes, n_hidden)) # K x m\n",
    "        self.b1 = np.zeros((n_hidden,1))\n",
    "        self.b2 = np.zeros((n_classes,1))\n",
    "        \n",
    "        \n",
    "\n",
    "    def normalise(self, train_X, val_X, test_X):\n",
    "        ''' X has shape (d,n) where d = dimensionality of each image, n is number of images '''\n",
    "        mean = np.mean(train_X, axis=1)\n",
    "        std = np.std(train_X, axis=1)\n",
    "        original_shape = train_X.shape\n",
    "        # apply same transformation to all of the datasets using params from train set\n",
    "        def _normalise_helper(a, m, s):\n",
    "            return ((a.T - m.T) / s.T).T\n",
    "        \n",
    "        train_X = _normalise_helper(train_X, mean, std)\n",
    "        val_X = _normalise_helper(val_X, mean, std)\n",
    "        test_X = _normalise_helper(test_X, mean, std)\n",
    "        return train_X, val_X, test_X\n",
    "\n",
    "    \n",
    "    def forward_pass(self, X, W1, W2, b1, b2):\n",
    "        S1 = np.dot(W1, X) + b1\n",
    "        H = np.where(S1 >= 0, s1, 0) # apply RELU activation\n",
    "        S = np.dot(W2,H) + b2\n",
    "        P = self.softmax(S) # probabiliites\n",
    "        predictions = np.argmax(P, axis=0)\n",
    "        return H, P\n",
    "    \n",
    "\n",
    "    def softmax(self, x):\n",
    "#         return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        # deal with overflow problem\n",
    "        return np.exp(x - np.max(x, axis=0)) / np.exp(x - np.max(x, axis=0)).sum(axis=0)\n",
    "    \n",
    "    \n",
    "    def evaluate_classifier(self, X, W1, W2, b1, b2):\n",
    "        H, P = self.forward_pass(X, W1, W2, b1, b2)\n",
    "        predictions = np.argmax(P, axis=0)\n",
    "        return predictions\n",
    "\n",
    "    def compute_accuracy(self, X, Y):\n",
    "        ''' X is data (dim, N), y is gt (C, N), W is weight matrix, b is bias, Y is 1hot encoded labels'''\n",
    "        pred = self.evaluate_classifier(X, self.W1, self.W2, self.b1, self.b2)\n",
    "        lbls = np.argmax(Y, axis=0)\n",
    "        accuracy = np.mean(pred == lbls)\n",
    "        return pred, accuracy\n",
    "    \n",
    "\n",
    "    def compute_cost(self, X, Y, W1, W2, b1, b2):\n",
    "        ''' \n",
    "            X: dxn (dimensionality by # images)\n",
    "            Y: Kxn (no. classes one-hot encoded by # images)\n",
    "            J: scalar corresponding to sum of loss of ntwks predictions of X relative to gt labels \n",
    "        '''\n",
    "        P, _ = self.evaluate_classifier(X, W1, W2, b1, b2)\n",
    "        N = X.shape[1]\n",
    "        #  loss function + regularisation term\n",
    "#             loss = -np.trace(Y*np.log(P)) / N\n",
    "        lcross = -np.sum(Y*np.log(P)) / N\n",
    "        J = lcross + self.lamda * (np.sum(W1**2) + np.sum(W2**2))\n",
    "\n",
    "        return J\n",
    "        \n",
    "\n",
    "    \n",
    "    def compute_gradients(self, X, Y, W1, W2, b1, b2):\n",
    "        ''' computes gradients of the cost function wrt W and b for batch X '''\n",
    "        N = X.shape[1]\n",
    "\n",
    "        # forward pass\n",
    "        H, P = self.evaluate_classifier(X, W1, W2, b1, b2)\n",
    "        \n",
    "        # backward pass\n",
    "        G = -(Y - P)\n",
    "       \n",
    "    \n",
    "        # J = L(D,W,b) + lamda|W|^2\n",
    "        # dJ/dW = dL/dW + 2 lambda |W|    \n",
    "#         grad_W = (np.dot(G, X.T) /  N) + 2 * self.lamda * W    \n",
    "#         grad_b = np.sum(G, axis=1) / N\n",
    "#         grad_b = grad_b.reshape((grad_b.shape[0],1))\n",
    "        \n",
    "        \n",
    "        grad_W2 =  np.dot(G, H.T) / N + 2 * self.lamda * W2 \n",
    "        grad_b2 =  np.sum(G, axis=1) / N\n",
    "        grad_b2 = grad_b2.reshape((grad_b2.shape[0],1))\n",
    "        # propagate gradient back through second layer\n",
    "        G = np.dot(W2.T, G)\n",
    "        Ind = H > 0\n",
    "        G = G * H # check this does what you want\n",
    "        grad_W1 =  np.dot(G, X.T) / N + 2 * self.lamda * W1 \n",
    "        grad_b1 =  np.sum(G, axis=1) / N\n",
    "        grad_b1 = grad_b1.reshape((grad_b1.shape[0],1))\n",
    "\n",
    "\n",
    "        np.sum(G, axis=1) / N\n",
    "        \n",
    "        return grad_W1, grad_W2, grad_b1, grad_b2\n",
    "    \n",
    "    \n",
    "    def train(self, X, Y, random_shuffle=False, val_X=None, val_Y=None, get_accuracies_costs=False, epoch_jump=1):\n",
    "        n = X.shape[1]\n",
    "        number_of_batches = int(n / self.batch_size)\n",
    "        indices = np.arange(X.shape[1])\n",
    "        if random_shuffle:\n",
    "            print('Randomly shuffling')\n",
    "        print('Loss function', self.loss)\n",
    "        \n",
    "        accuracies = {'train': [], 'val': []}\n",
    "        costs = {'train': [], 'val': []}\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "#             if (epoch % 10) == 0:\n",
    "#                 print('epoch', epoch)\n",
    "            if random_shuffle:\n",
    "                np.random.shuffle(indices)\n",
    "                X = np.take(X, indices, axis=1)\n",
    "                Y = np.take(Y, indices, axis=1)\n",
    "                \n",
    "            for j in range(number_of_batches):\n",
    "                j_start = j * self.batch_size\n",
    "                j_end = (j+1) * self.batch_size\n",
    "                Xbatch = X[:, j_start:j_end]\n",
    "                Ybatch = Y[:, j_start:j_end]\n",
    "    \n",
    "                # Perform MiniBatch Gradient Descent\n",
    "                grad_W1, grad_W2, grad_b1, grad_b2 = self.compute_gradients(Xbatch, Ybatch, self.W1, self.W2, self.b1, self.b2)\n",
    "                self.W1 -= self.eta * grad_W1\n",
    "                self.W2 -= self.eta * grad_W2\n",
    "                self.b1 -= self.eta *  grad_b1\n",
    "                self.b2 -= self.eta *  grad_b2\n",
    "\n",
    "            if get_accuracies_costs and epoch % epoch_jump == 0 or (epoch == self.n_epochs-1):\n",
    "                _, train_accuracy = self.compute_accuracy(X, Y)\n",
    "                _, val_accuracy = self.compute_accuracy(val_X, val_Y)\n",
    "                accuracies['train'].append(train_accuracy)\n",
    "                accuracies['val'].append(val_accuracy)\n",
    "                train_cost = self.compute_cost(X, Y, self.W, self.b)\n",
    "                val_cost = self.compute_cost(val_X, val_Y, self.W, self.b)\n",
    "                costs['train'].append(train_cost)\n",
    "                costs['val'].append(val_cost)\n",
    "                \n",
    "        return accuracies, costs\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data and normalise\n",
    "CIFARDATA = CIFAR10Data(dataset_dir='../datasets/cifar-10-batches-py/')\n",
    "train_X, train_Y = CIFARDATA.load_batch('data_batch_1')\n",
    "val_X, val_Y = CIFARDATA.load_batch('data_batch_2')\n",
    "test_X, test_Y = CIFARDATA.load_batch('test_batch')\n",
    "# datasets = [train_X, train_Y, val_X, val_Y, test_X , test_Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OneLayerClassifer(n_classes, input_dim, params['batch_size'], params['eta'], params['n_epochs'], params['lamda'], loss=loss, delta=delta)\n",
    "train_X, val_X, test_X = clf.normalise(train_X, val_X, test_X)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-anns",
   "language": "python",
   "name": "venv-anns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
