{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CIFAR10Data, plot_costs_accuracies\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Layer Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_dim, output_dim, activation='relu', batch_normalisation=False, sig=0.1):\n",
    "        if batch_normalisation:\n",
    "            # initialise weights to be normally distributed with sigmas equal to sig at each layer\n",
    "            self.W = np.random.normal(loc=0, scale=sig, size=(output_dim, input_dim)) \n",
    "        else:\n",
    "            # use He initialization \n",
    "            self.W = np.random.normal(loc=0, scale=1/np.sqrt(input_dim), size=(output_dim, input_dim)) \n",
    "        self.b = np.zeros((output_dim,1))\n",
    "        self.activation_fn = activation # should be softmax for last layer in definition\n",
    "        self.apply_bn = batch_normalisation\n",
    "        self.S_hat = None # batch normalised scores (pre scale and shift)\n",
    "\n",
    "        \n",
    "    def activation(self, S):\n",
    "        if self.activation_fn == 'relu':\n",
    "            return self.relu(S) \n",
    "        elif self.activation_fn == 'softmax':\n",
    "            return self.softmax(S)\n",
    "        else:\n",
    "            raise Exception('activation function invalid') \n",
    "    \n",
    "    \n",
    "    def relu(self, X):\n",
    "        return np.where(X >= 0, X, 0)\n",
    "    \n",
    "    \n",
    "    def softmax(self, X):\n",
    "        return np.exp(X) / np.sum(np.exp(X), axis=0)\n",
    "    \n",
    "    \n",
    "    def batch_normalise(self, S, means, sigmas, eps=0.0001):\n",
    "        print(S.shape)\n",
    "        print(len(means))\n",
    "        print(len(sigmas))\n",
    "        diag = np.zeros(len(sigmas), len(sigmas))\n",
    "        diag = np.fill_diagonal(diag, sigmas+eps)\n",
    "        S_hat = np.dot(diag, S-means)\n",
    "        self.S_hat = S_hat\n",
    "        return S_hat\n",
    "    \n",
    "    \n",
    "    def forward_pass(self, X, last_layer=False):\n",
    "        S = np.dot(self.W, X) \n",
    "        S += self.b\n",
    "        self.S = S\n",
    "\n",
    "        if self.apply_bn and not last_layer:\n",
    "            print('forward pass with bn')\n",
    "            means = np.sum(S, axis=0) / S.shape[1] \n",
    "            sigmas =  np.sum((S-means)**2) / S.shape[1]\n",
    "            self.means = means\n",
    "            self.sigmas = sigmas\n",
    "            S_hat = self.batch_normalise(S, means, sigmas)\n",
    "            S_scaled = np.multiply(self.gamma, S_hat) + self.beta # to do sort these out\n",
    "            self.S_scaled = S_scaled\n",
    "            return self.activation(S_scaled)\n",
    "        else:\n",
    "            return self.activation(S)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def update_gradients(self, eta, grad_W, grad_b, grad_gamma=None, grad_beta=None):\n",
    "        self.W -= (eta * grad_W)\n",
    "        self.b -= (eta * grad_b)\n",
    "        if grad_gamma is not None:\n",
    "            self.gamma -= (eta * grad_gamma)\n",
    "        if grad_beta is not None:\n",
    "            self.beta -= (eta * grad_beta)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLayerClassifier:\n",
    "    def __init__(self, batch_size=100, eta=0.001, n_cycles=2, lamda=0, cyclical_lr=False, n_s=None, eta_min=None, eta_max=None, lr_range_test=False, dropout_thresh=None, batch_normalisation=False):\n",
    "        self.batch_size = batch_size # number of images in a batch\n",
    "        \n",
    "        self.lamda = lamda\n",
    "        \n",
    "        self.cyclical_lr = cyclical_lr\n",
    "        if cyclical_lr:\n",
    "            self.batch_size = 100\n",
    "            self.l = 0\n",
    "            self.n_s = n_s # step size\n",
    "            self.eta_min = eta_min\n",
    "            self.eta_max = eta_max\n",
    "            self.plot_every_n_steps = int(2*n_s/10)\n",
    "            self.n_cycles = n_cycles\n",
    "            \n",
    "        else:\n",
    "            self.eta = eta\n",
    "        \n",
    "        if batch_normalisation:\n",
    "            self.forward_pass = self.forward_pass_w_bn\n",
    "            self.backward_pass = self.backward_pass_w_bn\n",
    "            self.batch_normalisation = batch_normalisation\n",
    "            \n",
    "        self.lr_range_test = lr_range_test # whether to perform learning rate range test to see what eta_min and eta_max should be\n",
    "        self.plot_every_n_steps = batch_size \n",
    "        self.dropout_thresh = dropout_thresh # proportion of nodes to switch off ( =(1-p) from slides)\n",
    "        \n",
    "        # initialise layers\n",
    "        self.layers = None\n",
    "    \n",
    "    def add_layer(self, input_dim, output_dim, activation='relu'):\n",
    "        # add a new layer\n",
    "        if self.layers is None:\n",
    "            self.layers = []\n",
    "        self.layers.append(Layer(input_dim, output_dim, activation, batch_normalisation=self.batch_normalisation))\n",
    "        \n",
    "\n",
    "    def normalise(self, train_X, val_X, test_X):\n",
    "        ''' X has shape (d,n) where d = dimensionality of each image, n is number of images '''\n",
    "        mean = np.mean(train_X, axis=1)\n",
    "        std = np.std(train_X, axis=1)\n",
    "        original_shape = train_X.shape\n",
    "        # apply same transformation to all of the datasets using params from train set\n",
    "        def _normalise_helper(a, m, s):\n",
    "            return ((a.T - m.T) / s.T).T\n",
    "        \n",
    "        train_X = _normalise_helper(train_X, mean, std)\n",
    "        val_X = _normalise_helper(val_X, mean, std)\n",
    "        test_X = _normalise_helper(test_X, mean, std)\n",
    "        return train_X, val_X, test_X\n",
    "\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        print('normal')\n",
    "        H = np.copy(X)\n",
    "        Xs = []\n",
    "        for layer in self.layers[:-1]: # loop layers 1...k-1\n",
    "            H = layer.forward_pass(H)\n",
    "            Xs.append(H)\n",
    "        # apply softmax to last layer instead of relu\n",
    "        last_layer = self.layers[-1]\n",
    "        P = last_layer.forward_pass(H, last_layer=True)\n",
    "        assert len(Xs) == len(self.layers) - 1 # k -1\n",
    "        return Xs, P\n",
    "    \n",
    "    \n",
    "    def forward_pass_w_bn(self, X):\n",
    "        H = np.copy(X)\n",
    "        Xs = []\n",
    "        means = []\n",
    "        sigmas = []\n",
    "        for layer in self.layers[:-1]: # loop layers 1...k-1\n",
    "            H = layer.forward_pass(H)\n",
    "            Xs.append(H)\n",
    "        # apply softmax to last layer instead of relu\n",
    "        last_layer = self.layers[-1]\n",
    "        P = last_layer.forward_pass(H, last_layer=True)\n",
    "        assert len(Xs) == len(self.layers) - 1 # k -1\n",
    "        return Xs, P\n",
    "    \n",
    "\n",
    "    def compute_accuracy(self, X, Y):\n",
    "        ''' X is data (dim, N), y is gt (C, N), W is weight matrix, b is bias, Y is 1hot encoded labels'''\n",
    "        _, P = self.forward_pass(X)\n",
    "        pred = np.argmax(P, axis=0)\n",
    "        lbls = np.argmax(Y, axis=0)\n",
    "        accuracy = np.mean(pred == lbls)\n",
    "        return pred, accuracy\n",
    "    \n",
    "\n",
    "    def compute_cost(self, X, Y, W1, W2, b1, b2): # TODO\n",
    "        ''' \n",
    "            X: dxn (dimensionality by # images)\n",
    "            Y: Kxn (no. classes one-hot encoded by # images)\n",
    "            J: scalar corresponding to sum of loss of ntwks predictions of X relative to gt labels \n",
    "        '''\n",
    "        _, P = self.forward_pass(X)\n",
    "        N = X.shape[1]\n",
    "        loss = -np.sum(Y*np.log(P)) / N\n",
    "        cost = loss + self.lamda * (np.sum(W1**2) + np.sum(W2**2))\n",
    "    \n",
    "        return loss, cost\n",
    "        \n",
    "    \n",
    "    def backward_pass(self, X, Y, P):\n",
    "        G = -(Y - P)\n",
    "        k = len(self.layers)\n",
    "        \n",
    "        grad_Ws = []\n",
    "        grad_bs = []\n",
    "        \n",
    "        for l in range(k-1, 0, -1): # propagate gradient backwarsd to first layer\n",
    "            layer = self.layers[l]\n",
    "            H = Xs[l-1]\n",
    "            grad_W = np.dot(G, H.T) / N + 2 * self.lamda * layer.W \n",
    "            grad_b = np.sum(G, axis=1) / N\n",
    "            grad_b = grad_b.reshape((grad_b.shape[0],1))\n",
    "\n",
    "            grad_Ws.append(grad_W)\n",
    "            grad_bs.append(grad_b)\n",
    "            G = np.dot(layer.W.T, G)\n",
    "            Ind = H > 0\n",
    "            G = np.multiply(G, Ind)\n",
    "            \n",
    "        # For first layer\n",
    "        layer = self.layers[0]\n",
    "        grad_W = np.dot(G, X.T) / N + 2 * self.lamda * layer.W \n",
    "        grad_b = np.sum(G, axis=1) / N\n",
    "        grad_b = grad_b.reshape((grad_b.shape[0],1))\n",
    "        grad_Ws.append(grad_W)\n",
    "        grad_bs.append(grad_b)\n",
    "        \n",
    "        # reverse order from first layer to last\n",
    "        return grad_Ws[::-1], grad_bs[::-1], None, None\n",
    "    \n",
    "    \n",
    "    def batch_norm_back_pass(self, G, S, means, sigmas):\n",
    "        pass\n",
    "    \n",
    "    def backward_pass_w_bn(self, X, Y, P):\n",
    "        G = -(Y - P)\n",
    "        k = len(self.layers)\n",
    "        \n",
    "        grad_Ws = []\n",
    "        grad_bs = []\n",
    "        grad_gammas = []\n",
    "        grad_betas = []\n",
    "        \n",
    "        for l in range(k-1, 0, -1): # propagate gradient backwarsd to first layer\n",
    "            layer = self.layers[l]\n",
    "            X_l = Xs[l-1]\n",
    "            # compute gradient for scale and offset parameters\n",
    "            grad_gamma = np.multiply(G, layer.S_hat) / N \n",
    "            grad_beta = np.sum(G, axis=1) / N\n",
    "            grad_beta = grad_shift.reshape((grad_shift.shape[0],1))\n",
    "            # propagate gradients through scale and shift\n",
    "            G = np.multiply(G, )\n",
    "            # propagate G through batch normalisation\n",
    "            G = self.batch_norm_back_pass(G, layer.S, layer.means, layer.sigmas)   \n",
    "            # calculate gradients of J wrt bias and weights\n",
    "            grad_W = np.dot(G, X_l.T) / N + 2 * self.lamda * layer.W \n",
    "            grad_b = np.sum(G, axis=1) / N\n",
    "            grad_b = grad_b.reshape((grad_b.shape[0],1))\n",
    "            \n",
    "            grad_Ws.append(grad_W)\n",
    "            grad_bs.append(grad_b)\n",
    "            grad_gammas.append(grad_gamma)\n",
    "            grad_betas.append(grad_beta)\n",
    "            \n",
    "            G = np.dot(layer.W.T, G)\n",
    "            Ind = H > 0\n",
    "            G = np.multiply(G, Ind)\n",
    "            \n",
    "        # For first layer\n",
    "        layer = self.layers[0]\n",
    "        grad_W = np.dot(G, X.T) / N + 2 * self.lamda * layer.W \n",
    "        grad_b = np.sum(G, axis=1) / N\n",
    "        grad_b = grad_b.reshape((grad_b.shape[0],1))\n",
    "        grad_Ws.append(grad_W)\n",
    "        grad_bs.append(grad_b)\n",
    "        \n",
    "        # reverse order from first layer to last\n",
    "        return grad_Ws[::-1], grad_bs[::-1], grad_gammas[::-1], grad_betas[::-1] \n",
    "    \n",
    "    \n",
    "    def compute_gradients(self, X, Y):\n",
    "        ''' computes gradients of the cost function wrt W and b for batch X '''\n",
    "        N = X.shape[1]\n",
    "\n",
    "        # forward pass, apply dropout if its set\n",
    "        Xs, P = self.forward_pass(X)\n",
    "        \n",
    "        # backward pass\n",
    "        grad_Ws, grad_bs, grad_gammas, grad_betas = self.backward_pass(X, Y, P)\n",
    "        \n",
    "        return grad_Ws, grad_bs, grad_gammas, grad_betas\n",
    "    \n",
    "   \n",
    "    def compute_cyclical_lr(self, t):\n",
    "        l = self.l\n",
    "        eta_min = self.eta_min\n",
    "        eta_max = self.eta_max\n",
    "        n_s = self.n_s\n",
    "        if t >= 2*l*n_s and t <= (2*l+1)*n_s:\n",
    "            eta = eta_min + (t-2*l*n_s)*(eta_max-eta_min)/n_s\n",
    "        elif t >= (2*l+1)*n_s and t <= 2*(l+1)*n_s:\n",
    "            eta = eta_max - (t - (2*l+1)*n_s)*(eta_max-eta_min)/n_s\n",
    "            \n",
    "        if t % (2*n_s) == 0 and t > 0: # update l every 2*n_s steps\n",
    "                self.l += 1\n",
    "        return eta\n",
    "    \n",
    "        \n",
    "        \n",
    "    def train(self, X, Y, random_shuffle=False, val_X=None, val_Y=None, get_accuracies_costs=False, max_lr=0.1, n_epochs=20):\n",
    "        n = X.shape[1]\n",
    "        \n",
    "        number_of_batches = int(n / self.batch_size)\n",
    "        \n",
    "        assert number_of_batches > 0\n",
    "        indices = np.arange(X.shape[1])\n",
    "        if random_shuffle:\n",
    "            print('Randomly shuffling')\n",
    "        \n",
    "        if self.cyclical_lr:\n",
    "            print('Cyclical learning rate')\n",
    "            n_epochs = int(self.n_s * 2 * self.n_cycles / number_of_batches)\n",
    "       \n",
    "        accuracies = {'train': [], 'val': []}\n",
    "        costs = {'train': [], 'val': []}\n",
    "        losses = {'train': [], 'val': []}\n",
    "        update_steps = []\n",
    "        \n",
    "        eta = self.eta if not self.cyclical_lr else None\n",
    "        t = 0 # each update step % 2*n_s\n",
    "        if self.lr_range_test:\n",
    "            eta = 0\n",
    "            \n",
    "            self.plot_every_n_step = int(n_epochs * number_of_batches / 20)\n",
    "            eta_incr = max_lr * self.plot_every_n_step / (n_epochs * number_of_batches)\n",
    "            \n",
    "        \n",
    "        for epoch in range(n_epochs):     \n",
    "            if random_shuffle:\n",
    "                np.random.shuffle(indices)\n",
    "                X = np.take(X, indices, axis=1)\n",
    "                Y = np.take(Y, indices, axis=1)\n",
    "                \n",
    "            for j in range(number_of_batches):\n",
    "                if self.cyclical_lr:\n",
    "                    eta = self.compute_cyclical_lr(t)\n",
    "                \n",
    "                \n",
    "                j_start = j * self.batch_size\n",
    "                j_end = (j+1) * self.batch_size\n",
    "                Xbatch = X[:, j_start:j_end]\n",
    "                Ybatch = Y[:, j_start:j_end]\n",
    "                \n",
    "                # Perform MiniBatch Gradient Descent\n",
    "                grads = self.compute_gradients(Xbatch, Ybatch)\n",
    "                for i in range(len(self.layers)): \n",
    "                    self.layers[i].update_gradients(eta, *grads) \n",
    "\n",
    "                if get_accuracies_costs and (t % self.plot_every_n_steps == 0):\n",
    "                    _, train_accuracy = self.compute_accuracy(X, Y)\n",
    "                    _, val_accuracy = self.compute_accuracy(val_X, val_Y)\n",
    "                    accuracies['train'].append(train_accuracy)\n",
    "                    accuracies['val'].append(val_accuracy)\n",
    "                    train_loss, train_cost = self.compute_cost(X, Y)\n",
    "                    val_loss, val_cost = self.compute_cost(val_X, val_Y)\n",
    "                    costs['train'].append(train_cost)\n",
    "                    costs['val'].append(val_cost)\n",
    "                    losses['train'].append(train_loss)\n",
    "                    losses['val'].append(val_loss)\n",
    "                    update_steps.append(t)\n",
    "            \n",
    "                \n",
    "                if self.lr_range_test and (t % self.plot_every_n_step == 0):\n",
    "                    update_steps.append(eta)\n",
    "                    _, train_accuracy = self.compute_accuracy(X, Y)\n",
    "                    _, val_accuracy = self.compute_accuracy(val_X, val_Y)\n",
    "                    accuracies['train'].append(train_accuracy)\n",
    "                    accuracies['val'].append(val_accuracy)\n",
    "                    eta += eta_incr\n",
    "                    \n",
    "                t += 1\n",
    "   \n",
    "\n",
    "        return accuracies, losses, costs, update_steps\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly shuffling\n",
      "Cyclical learning rate\n",
      "here\n",
      "forward pass with bn\n",
      "(50, 100)\n",
      "100\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e091e7294721>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_shuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-d538d6ffaaf4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y, random_shuffle, val_X, val_Y, get_accuracies_costs, max_lr, n_epochs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;31m# Perform MiniBatch Gradient Descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                 \u001b[0mgrad_Ws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_bs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_Ws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_bs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-d538d6ffaaf4>\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# forward pass, apply dropout if its set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-d538d6ffaaf4>\u001b[0m in \u001b[0;36mforward_pass_w_bn\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0msigmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# loop layers 1...k-1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mXs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# apply softmax to last layer instead of relu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-78b0cdd568d8>\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(self, X, last_layer)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0msigmas\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_normalise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-78b0cdd568d8>\u001b[0m in \u001b[0;36mactivation\u001b[0;34m(self, S)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_fn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_fn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'softmax'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-78b0cdd568d8>\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>=' not supported between instances of 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "n_s = 5 * 45000 / 100\n",
    "model = KLayerClassifier(batch_size=100, eta=0.005, n_cycles=2, lamda=.005, cyclical_lr=True, n_s=n_s, eta_min=10**-5, eta_max=0.1, batch_normalisation=True)\n",
    "train_X, val_X, test_X = model.normalise(train_X, val_X, test_X)\n",
    "hidden_nodes = [50, 30, 20, 20, 10, 10, 10, 10]\n",
    "model.add_layer(input_dim=train_X.shape[0], output_dim=hidden_nodes[0])\n",
    "\n",
    "for i, h in enumerate(hidden_nodes[:-1]):\n",
    "    model.add_layer(input_dim=hidden_nodes[i], output_dim=hidden_nodes[i+1])\n",
    "\n",
    "model.add_layer(input_dim=hidden_nodes[-1], output_dim=10, activation='softmax')\n",
    "model.train(train_X, train_Y, random_shuffle=True)\n",
    "_, test_accuracy = model.compute_accuracy(test_X, test_Y)\n",
    "print('test_accuracy', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data and normalise\n",
    "CIFARDATA = CIFAR10Data(dataset_dir='../datasets/cifar-10-batches-py/')\n",
    "train_X, train_Y = CIFARDATA.load_batch('data_batch_1')\n",
    "val_X, val_Y = CIFARDATA.load_batch('data_batch_2')\n",
    "test_X, test_Y = CIFARDATA.load_batch('test_batch')\n",
    "datasets = [train_X, train_Y, val_X, val_Y, test_X , test_Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in all data\n",
    "train_X, train_Y = CIFARDATA.load_batches(['data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4'])\n",
    "val_X, val_Y = CIFARDATA.load_batch('data_batch_5')\n",
    "train_X = np.hstack((train_X, val_X[:,:5000]))\n",
    "train_Y = np.hstack((train_Y, val_Y[:,:5000]))\n",
    "val_X = val_X[:,5000:]\n",
    "val_Y = val_Y[:,5000:]\n",
    "test_X, test_Y = CIFARDATA.load_batch('test_batch')\n",
    "datasets = [train_X, train_Y, val_X, val_Y, test_X , test_Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check gradients are correct for 3 layer classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check we can get the same performance for the 2-layer classifier from Assignment 2. We build a network with 2 layers, the hidden layer has 50 nodes. Activation function for first hidden layer is RELU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly shuffling\n",
      "Cyclical learning rate\n",
      "test_accuracy 0.5172\n"
     ]
    }
   ],
   "source": [
    "n_s = 5 * 45000 / 100\n",
    "model = KLayerClassifier(batch_size=100, eta=0.005, n_cycles=2, lamda=.01, cyclical_lr=True, n_s=n_s, eta_min=10**-5, eta_max=0.1)\n",
    "train_X, val_X, test_X = model.normalise(train_X, val_X, test_X)\n",
    "model.add_layer(input_dim=train_X.shape[0], output_dim=50)\n",
    "model.add_layer(input_dim=50, output_dim=10, activation='softmax')\n",
    "model.train(train_X, train_Y, random_shuffle=True)\n",
    "_, test_accuracy = model.compute_accuracy(test_X, test_Y)\n",
    "print('test_accuracy', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly shuffling\n",
      "Cyclical learning rate\n",
      "test_accuracy 0.5242\n"
     ]
    }
   ],
   "source": [
    "n_s = 5 * 45000 / 100\n",
    "model = KLayerClassifier(batch_size=100, eta=0.005, n_cycles=2, lamda=.005, cyclical_lr=True, n_s=n_s, eta_min=10**-5, eta_max=0.1)\n",
    "train_X, val_X, test_X = model.normalise(train_X, val_X, test_X)\n",
    "model.add_layer(input_dim=train_X.shape[0], output_dim=50)\n",
    "model.add_layer(input_dim=50, output_dim=10, activation='softmax')\n",
    "model.train(train_X, train_Y, random_shuffle=True)\n",
    "_, test_accuracy = model.compute_accuracy(test_X, test_Y)\n",
    "print('test_accuracy', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9-layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly shuffling\n",
      "Cyclical learning rate\n",
      "test_accuracy 0.4075\n"
     ]
    }
   ],
   "source": [
    "n_s = 5 * 45000 / 100\n",
    "model = KLayerClassifier(batch_size=100, eta=0.005, n_cycles=2, lamda=.005, cyclical_lr=True, n_s=n_s, eta_min=10**-5, eta_max=0.1)\n",
    "train_X, val_X, test_X = model.normalise(train_X, val_X, test_X)\n",
    "hidden_nodes = [50, 30, 20, 20, 10, 10, 10, 10]\n",
    "model.add_layer(input_dim=train_X.shape[0], output_dim=hidden_nodes[0])\n",
    "\n",
    "for i, h in enumerate(hidden_nodes[:-1]):\n",
    "    model.add_layer(input_dim=hidden_nodes[i], output_dim=hidden_nodes[i+1])\n",
    "\n",
    "model.add_layer(input_dim=hidden_nodes[-1], output_dim=10, activation='softmax')\n",
    "model.train(train_X, train_Y, random_shuffle=True)\n",
    "_, test_accuracy = model.compute_accuracy(test_X, test_Y)\n",
    "print('test_accuracy', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it is hard to train a deeper network well and performance drops with 9 layers. We will apply batch normalisation as a way to overcome this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a more exhaustive random search to find good values for the amount of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply dropout to your training if you have a high number of hidden nodes and you feel you need more regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has been empirically reported in several works that you get better perfor- mance by the final network if you apply batch normalization to the scores after the non-linear activation function has been applied. You could inves- tigate whether this is the case. You will have to update your forward and backward pass of the back-prop algorithm accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a more thorough search to find a good network architecture. Does making the network deeper improve performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-anns",
   "language": "python",
   "name": "venv-anns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
