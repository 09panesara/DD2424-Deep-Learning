{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CIFAR10Data, plot_costs_accuracies\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Layer Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init(self, input_dim, output_dim, activation='relu'):\n",
    "        self.W = np.random.normal(loc=0, scale=1/np.sqrt(input_dim), size=(output_dim, input_dim)) \n",
    "        self.b = np.zeros((output_dim,1))\n",
    "        self.activation = activation # should be softmax for last layer in definition\n",
    "#         self.eta = eta\n",
    "        pass\n",
    "    \n",
    "    def activation(self, S):\n",
    "        if self.activation == 'relu':\n",
    "            return self.relu(S) \n",
    "        elif self.activation == 'softmax':\n",
    "            return self.softmax()\n",
    "        else:\n",
    "            raise Exception('activation function invalid') \n",
    "    \n",
    "    def relu(self, X):\n",
    "        return np.where(X >= 0, X, 0)\n",
    "    \n",
    "    \n",
    "    def softmax(self, X):\n",
    "        return np.exp(X) / np.sum(np.exp(X), axis=0)\n",
    "    \n",
    "    \n",
    "    def forward_pass(self, X, last_layer=False):\n",
    "        S = np.dot(self.W, X) + self.b\n",
    "        return self.activation(S)\n",
    "        \n",
    "    def backward_pass(self):\n",
    "        pass\n",
    "    \n",
    "    def update_gradients(self, grad_W, grad_b, eta):\n",
    "        grad_W -= self.eta * \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLayerClassifer:\n",
    "    def __init__(self, input_dim, batch_size=100, eta=0.001, n_epochs=20, lamda=0, cyclical_lr=False, n_s=None, eta_min=None, eta_max=None, lr_range_test=False, dropout_thresh=None):\n",
    "        n_classes = 10\n",
    "        self.batch_size = batch_size # number of images in a batch\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lamda = lamda\n",
    "        \n",
    "        self.cyclical_lr = cyclical_lr\n",
    "        if cyclical_lr:\n",
    "            self.batch_size = 100\n",
    "            self.l = 0\n",
    "            self.n_s = n_s # step size\n",
    "            self.eta_min = eta_min\n",
    "            self.eta_max = eta_max\n",
    "            self.plot_every_n_steps = int(2*n_s/10)\n",
    "        else:\n",
    "            self.eta = eta\n",
    "        self.lr_range_test = lr_range_test # whether to perform learning rate range test to see what eta_min and eta_max should be\n",
    "        self.plot_every_n_steps = batch_size \n",
    "        self.n_hidden = n_hidden\n",
    "        self.dropout_thresh = dropout_thresh # proportion of nodes to switch off ( =(1-p) from slides)\n",
    "        \n",
    "        # initialise layers\n",
    "        self.layers = None\n",
    "    \n",
    "    def add_layer(self, input_dim, output_dim, activation='relu'):\n",
    "        # add a new layer\n",
    "        if self.layers is None:\n",
    "            self.layers = []\n",
    "        self.layers.append(Layer(input_dim, output_dim, activation))\n",
    "        \n",
    "\n",
    "    def normalise(self, train_X, val_X, test_X):\n",
    "        ''' X has shape (d,n) where d = dimensionality of each image, n is number of images '''\n",
    "        mean = np.mean(train_X, axis=1)\n",
    "        std = np.std(train_X, axis=1)\n",
    "        original_shape = train_X.shape\n",
    "        # apply same transformation to all of the datasets using params from train set\n",
    "        def _normalise_helper(a, m, s):\n",
    "            return ((a.T - m.T) / s.T).T\n",
    "        \n",
    "        train_X = _normalise_helper(train_X, mean, std)\n",
    "        val_X = _normalise_helper(val_X, mean, std)\n",
    "        test_X = _normalise_helper(test_X, mean, std)\n",
    "        return train_X, val_X, test_X\n",
    "\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        H = np.copy(X)\n",
    "        Xs = []\n",
    "        for layer in self.layers[:-1]: # loop layers 1...k-1\n",
    "            H = layer.forward_pass(H)\n",
    "            Xs.append(H)\n",
    "        # apply softmax to last layer instead of relu\n",
    "        P = layer.forward_pass(H, last_layer='True')\n",
    "        assert len(Xs) = len(self.layers) - 1 # k -1\n",
    "        return Xs, P\n",
    "\n",
    "\n",
    "    def compute_accuracy(self, X, Y):\n",
    "        ''' X is data (dim, N), y is gt (C, N), W is weight matrix, b is bias, Y is 1hot encoded labels'''\n",
    "        _, P = self.forward_pass(X)\n",
    "        pred = np.argmax(P, axis=0)\n",
    "        lbls = np.argmax(Y, axis=0)\n",
    "        accuracy = np.mean(pred == lbls)\n",
    "        return pred, accuracy\n",
    "    \n",
    "\n",
    "    def compute_cost(self, X, Y, W1, W2, b1, b2): # TODO\n",
    "        ''' \n",
    "            X: dxn (dimensionality by # images)\n",
    "            Y: Kxn (no. classes one-hot encoded by # images)\n",
    "            J: scalar corresponding to sum of loss of ntwks predictions of X relative to gt labels \n",
    "        '''\n",
    "        _, P = self.forward_pass(X)\n",
    "        N = X.shape[1]\n",
    "        loss = -np.sum(Y*np.log(P)) / N\n",
    "        cost = loss + self.lamda * (np.sum(W1**2) + np.sum(W2**2))\n",
    "    \n",
    "        return loss, cost\n",
    "        \n",
    "    \n",
    "    \n",
    "    def compute_gradients(self, X, Y):\n",
    "        ''' computes gradients of the cost function wrt W and b for batch X '''\n",
    "        N = X.shape[1]\n",
    "\n",
    "        # forward pass, apply dropout if its set\n",
    "        Xs, P = self.forward_pass(X)\n",
    "        \n",
    "        # backward pass\n",
    "        G = -(Y - P)\n",
    "        k = len(self.layers)\n",
    "        \n",
    "        grad_Ws = []\n",
    "        grad_bs = []\n",
    "        \n",
    "        for l in range(k-1, 1, -1): # propagate gradient backwarsd to first layer\n",
    "            layer = self.layers[l]\n",
    "            H = Xs[l-1]\n",
    "            grad_W = np.dot(G, H.T) / N + 2 * layer.lamda * layer.W \n",
    "            grad_b = np.sum(G, axis=1) / N\n",
    "            grad_Ws.append(grad_W)\n",
    "            grad_bs.append(grad_b)\n",
    "            G = np.dot(layer.W.T, G)\n",
    "            Ind = H > 0\n",
    "            G = np.multiply(G, Ind)\n",
    "            \n",
    "        # For first layer\n",
    "        layer = self.layers[0]\n",
    "        grad_W = np.dot(G, X.T) / N + 2 * layer.lamda * layer.W \n",
    "        grad_b = np.sum(G, axis=1) / N\n",
    "        grad_Ws.append(grad_W)\n",
    "        grad_bs.append(grad_b)\n",
    "        \n",
    "        return grad_Ws, grad_bs\n",
    "    \n",
    "   \n",
    "    def compute_cyclical_lr(self, t):\n",
    "        l = self.l\n",
    "        eta_min = self.eta_min\n",
    "        eta_max = self.eta_max\n",
    "        n_s = self.n_s\n",
    "        if t >= 2*l*n_s and t <= (2*l+1)*n_s:\n",
    "            eta = eta_min + (t-2*l*n_s)*(eta_max-eta_min)/n_s\n",
    "        elif t >= (2*l+1)*n_s and t <= 2*(l+1)*n_s:\n",
    "            eta = eta_max - (t - (2*l+1)*n_s)*(eta_max-eta_min)/n_s\n",
    "            \n",
    "        if t % (2*n_s) == 0 and t > 0: # update l every 2*n_s steps\n",
    "                self.l += 1\n",
    "        return eta\n",
    "    \n",
    "        \n",
    "        \n",
    "    def train(self, X, Y, random_shuffle=False, val_X=None, val_Y=None, get_accuracies_costs=False, max_lr=0.1):\n",
    "        n = X.shape[1]\n",
    "        \n",
    "        number_of_batches = int(n / self.batch_size)\n",
    "        assert number_of_batches > 0\n",
    "        indices = np.arange(X.shape[1])\n",
    "        if random_shuffle:\n",
    "            print('Randomly shuffling')\n",
    "        \n",
    "        if self.cyclical_lr:\n",
    "            print('Cyclical learning rate')\n",
    "       \n",
    "        accuracies = {'train': [], 'val': []}\n",
    "        costs = {'train': [], 'val': []}\n",
    "        losses = {'train': [], 'val': []}\n",
    "        update_steps = []\n",
    "        \n",
    "        eta = self.eta if not self.cyclical_lr else None\n",
    "        t = 0 # each update step % 2*n_s\n",
    "        if self.lr_range_test:\n",
    "            eta = 0\n",
    "            \n",
    "            self.plot_every_n_step = int(self.n_epochs * number_of_batches / 20)\n",
    "            eta_incr = max_lr * self.plot_every_n_step / (self.n_epochs * number_of_batches)\n",
    "            \n",
    "        \n",
    "        for epoch in range(self.n_epochs):     \n",
    "            if random_shuffle:\n",
    "                np.random.shuffle(indices)\n",
    "                X = np.take(X, indices, axis=1)\n",
    "                Y = np.take(Y, indices, axis=1)\n",
    "                \n",
    "            for j in range(number_of_batches):\n",
    "                if self.cyclical_lr:\n",
    "                    eta = self.compute_cyclical_lr(t)\n",
    "                \n",
    "                \n",
    "                j_start = j * self.batch_size\n",
    "                j_end = (j+1) * self.batch_size\n",
    "                Xbatch = X[:, j_start:j_end]\n",
    "                Ybatch = Y[:, j_start:j_end]\n",
    "                \n",
    "                # Perform MiniBatch Gradient Descent\n",
    "                grad_Ws, grad_bs = self.compute_gradients(Xbatch, Ybatch)\n",
    "                for i in range(len(self.layers)): \n",
    "                    self.layers[i].update_gradients(grad_Ws[i], grad_bs[i], self.eta) \n",
    "\n",
    "                if get_accuracies_costs and (t % self.plot_every_n_steps == 0):\n",
    "                    _, train_accuracy = self.compute_accuracy(X, Y)\n",
    "                    _, val_accuracy = self.compute_accuracy(val_X, val_Y)\n",
    "                    accuracies['train'].append(train_accuracy)\n",
    "                    accuracies['val'].append(val_accuracy)\n",
    "                    train_loss, train_cost = self.compute_cost(X, Y)\n",
    "                    val_loss, val_cost = self.compute_cost(val_X, val_Y)\n",
    "                    costs['train'].append(train_cost)\n",
    "                    costs['val'].append(val_cost)\n",
    "                    losses['train'].append(train_loss)\n",
    "                    losses['val'].append(val_loss)\n",
    "                    update_steps.append(t)\n",
    "            \n",
    "                \n",
    "                if self.lr_range_test and (t % self.plot_every_n_step == 0):\n",
    "                    update_steps.append(eta)\n",
    "                    _, train_accuracy = self.compute_accuracy(X, Y)\n",
    "                    _, val_accuracy = self.compute_accuracy(val_X, val_Y)\n",
    "                    accuracies['train'].append(train_accuracy)\n",
    "                    accuracies['val'].append(val_accuracy)\n",
    "                    eta += eta_incr\n",
    "                    \n",
    "                t += 1\n",
    "   \n",
    "\n",
    "        return accuracies, losses, costs, update_steps\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data and normalise\n",
    "CIFARDATA = CIFAR10Data(dataset_dir='../datasets/cifar-10-batches-py/')\n",
    "train_X, train_Y = CIFARDATA.load_batch('data_batch_1')\n",
    "val_X, val_Y = CIFARDATA.load_batch('data_batch_2')\n",
    "test_X, test_Y = CIFARDATA.load_batch('test_batch')\n",
    "datasets = [train_X, train_Y, val_X, val_Y, test_X , test_Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check gradients are correct for 3 layer classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a more exhaustive random search to find good values for the amount of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply dropout to your training if you have a high number of hidden nodes and you feel you need more regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has been empirically reported in several works that you get better perfor- mance by the final network if you apply batch normalization to the scores after the non-linear activation function has been applied. You could inves- tigate whether this is the case. You will have to update your forward and backward pass of the back-prop algorithm accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a more thorough search to find a good network architecture. Does making the network deeper improve performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-anns",
   "language": "python",
   "name": "venv-anns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
