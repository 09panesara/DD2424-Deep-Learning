{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Back Propagation and AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, number_states=100, eta=0.1, seq_length=25):\n",
    "        '''\n",
    "        Inputs:\n",
    "            K: output size\n",
    "            number_states: dimensionality of hidden state\n",
    "            eta: learning rate\n",
    "            seq_length: length of sequences used in training\n",
    "        Initialises weight matrices and params\n",
    "        '''\n",
    "        sigma = 0.01\n",
    "        self.U = np.random.randn(m,K)*sigma\n",
    "        self.W = np.random.randn(m,m)*sigma\n",
    "        self.V = np.random.randn(K,m)*sigma\n",
    "        self.eta = eta    \n",
    "        self.b = np.zeros((m,1))\n",
    "        self.c = np.zeros((K,1))\n",
    "        self.number_states = 100 # m\n",
    "        \n",
    "    \n",
    "    def init_one_hot(self, chars):\n",
    "        ''' takes in list set of all characters in test'''\n",
    "        self.output_size = len(chars)\n",
    "        self.chars = chars\n",
    "        self.one_hot_encoder = preprocessing.LabelBinarizer()\n",
    "        self.one_hot_encoder.fit(chars)\n",
    "    \n",
    "    \n",
    "    def one_hot_encode(self, seq):\n",
    "        ''' \n",
    "        Inputs:\n",
    "            seq: list of characters to one hot encode\n",
    "        Returns:\n",
    "            Y: (output_size, T) where output_size is dimensionality of one hot encoding, T is len(seq)\n",
    "        '''\n",
    "        seq_list = []\n",
    "        seq_list[:0] = seq \n",
    "        Y = self.one_hot_encoder.transform(seq_list).T\n",
    "        assert Y.shape == (self.K, len(seq))\n",
    "        return Y\n",
    "\n",
    "    \n",
    "    def one_hot_to_characters(self, Y):\n",
    "        ''' \n",
    "        Inputs:\n",
    "            Y: Array of one hot encodings (output size, T)\n",
    "        Returns:\n",
    "             and converts back into characters '''\n",
    "        assert Y.shape[0] == self.K\n",
    "        chars_list = self.one_hot_encoder.inverse_transform(Y.T)\n",
    "        seq = ''.join(chars_list)\n",
    "        return seq\n",
    "    \n",
    "\n",
    "    def softmax(self, X):\n",
    "        return np.exp(X) / np.sum(np.exp(X), axis=0)\n",
    "    \n",
    "    \n",
    "    def loss(self, Y, P):\n",
    "        ''' \n",
    "        Inputs:\n",
    "            Y: labels for each character in sequence (length T)\n",
    "            P: probabilities for each character in sequence (length T)\n",
    "        Returns:\n",
    "            Sum of cross entropy loss for each character in sequence'''\n",
    "        -np.sum(log())\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def sample_character(self, P, n):\n",
    "        '''\n",
    "        Inputs:\n",
    "            P: output probability distribution used to sample a character\n",
    "            n: number of characters to installed\n",
    "        Returns:\n",
    "            Y: Knx matrix containing one-hot encoding of n sampled characters\n",
    "        '''\n",
    "        # sample n characters according to probability distribution\n",
    "        c = np.random.choice(numpy.arange(len(P)), size=n, p=P)\n",
    "        print(p)\n",
    "        Y = self.one_hot_encode(c)\n",
    "        return Y\n",
    "        \n",
    "        \n",
    "    def synthesise_sequence(self, x0, h0):\n",
    "        ''' Ex0.3\n",
    "        Inputs:\n",
    "            x0: vector for first (dummy) input to RNN\n",
    "            h0: hidden state at time 0\n",
    "        Returns:\n",
    "            synthesises sequence X using current parameters\n",
    "        '''\n",
    "        for t in range(T):\n",
    "            # generate next input x_t from current x\n",
    "            x_t = np.copy(x0)\n",
    "            a_t = self.W @ h_tm1 + self.U @ x_t + self.b\n",
    "            h_t = np.tanh(a_t)\n",
    "            o_t = self.V @ h_t + self.c\n",
    "            # probability for each possible character\n",
    "            p_t = self.softmax(o_t)\n",
    "            \n",
    "            # update params for next iteration\n",
    "            h_tm1 = h_t\n",
    "            x_t = self.sample_character(p_t, 1)\n",
    "            \n",
    "            \n",
    "    def backward_pass(self, X):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward(self, X, h_tm1):\n",
    "        '''\n",
    "        Completes a forward pass of RNN network\n",
    "        Inputs:\n",
    "            X: Sequence (self.output_size, T)\n",
    "            h_tm1: previous state\n",
    "        Returns:\n",
    "            loss: cross-entropy loss of sequence\n",
    "            P: probabilities across timesteps\n",
    "            H: hidden states across timesteps \n",
    "        '''\n",
    "        T = X.shape[1] # length of sequence\n",
    "        P = np.zeros((T, self.output_size))\n",
    "        H = np.zeros((T+1, self.number_states))\n",
    "        \n",
    "        for t in range(T):\n",
    "            x_t = np.copy(x0)\n",
    "            a_t = self.W @ h_tm1 + self.U @ x_t + self.b\n",
    "            h_t = np.tanh(a_t)\n",
    "            o_t = self.V @ h_t + self.c\n",
    "            # probability for each possible character\n",
    "            p_t = self.softmax(o_t)\n",
    "            \n",
    "            # store in P, H for use in backward algorithm\n",
    "            P[t,:] = p_t\n",
    "            H[t,:] = h_tm1\n",
    "            \n",
    "            h_tm1 = np.copy(h_t)\n",
    "    \n",
    "        return loss, P, H\n",
    "    \n",
    "    \n",
    "    def train(self, X, Y):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asda'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_data = open('goblet_book.txt','r').read()\n",
    "book_chars = list(set(book_data))\n",
    "rnn = RNN()\n",
    "rnn.init_one_hot(book_chars)\n",
    "\n",
    "# test one hot encoding\n",
    "y = rnn.one_hot_encode('asda')\n",
    "recovered = rnn.one_hot_to_characters(y)\n",
    "recovered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking gradients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a subset of book_data to debug the model on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "seq_length = 25\n",
    "X_chars = book_data[:seq_length]\n",
    "# label for an input character is the next character in tthe book\n",
    "Y_chars = book_data[1:seq_length+1]\n",
    "X = rnn.one_hot_encode(X_chars) # (K, seq_length)\n",
    "Y = rnn.one_hot_encode(Y_chars) # (K, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your RNN using AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesising Donald Trump tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-anns",
   "language": "python",
   "name": "venv-anns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
